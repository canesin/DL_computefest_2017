{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and restoring states\n",
    "\n",
    "In this notebook we see how to save and restore the variables in a computational graph. This can be used for example when working with networks that require week to be trained. \n",
    "We can also save and load only some arbitrary network variables. This can be used when doing Transfer Learning, which means to take a partial initialization of a network to train another network with different ending layers.\n",
    "\n",
    "The network implemented here is the simple one used in the CNN notebook,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "import os.path\n",
    "import math\n",
    "BATCH_SIZE = 100\n",
    "RESTORE_OPT = True\n",
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def conv_weights(filters_size, channels_size, name):\n",
    "    shape = filters_size + channels_size\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1), name=name)\n",
    "\n",
    "def conv(x, W, stride, name):\n",
    "    strides_shape = [1, stride, stride, 1]\n",
    "    return tf.nn.conv2d(x, W, strides_shape, padding='SAME', name=name)\n",
    "\n",
    "def pool(x, size, stride, name):\n",
    "    pool_shape = [1] + size + [1]\n",
    "    strides_shape = [1, stride, stride, 1]\n",
    "    return tf.nn.max_pool(x, pool_shape, strides_shape, padding='SAME', name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 28 * 28], name='input_images')\n",
    "y_ = tf.placeholder(tf.float32, [None, 10], name='labels')\n",
    "input_images = tf.reshape(x, [-1, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W1 = conv_weights([3, 3], [1, 32], 'L1_weights')\n",
    "b1 = tf.Variable(tf.constant(0.1, shape=[32]), name='L1_biases')\n",
    "c1 = conv(input_images, W1, stride=2, name='L1_conv')\n",
    "h1 = tf.nn.relu(tf.nn.bias_add(c1, b1), name='L1_ReLU')\n",
    "p1 = pool(h1, size=[2, 2], stride=2, name='L1_pool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W2 = tf.Variable(tf.truncated_normal([7 * 7 * 32, 10], stddev=1 / math.sqrt(7 * 7 * 32)), name='L2_weights')\n",
    "b2 = tf.Variable(tf.constant(0.1, shape=[10]), name='L2_biases')\n",
    "p1_flat = tf.reshape(p1, [-1, 7 * 7 * 32])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logits = tf.matmul(p1_flat, W2) + b2\n",
    "y = tf.nn.softmax(logits, name='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]), name='cross_entropy')\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1), name='correct_prediction')\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='train_accuracy')\n",
    "\n",
    "sess = tf.InteractiveSession()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_dir = 'check/'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"check/checkpoint\") and RESTORE_OPT:\n",
    "    saver.restore(sess, \"check/checkp\")\n",
    "    print (\"Model restored\")\n",
    "    init_new_vars_op = tf.variables_initializer([W2,b2])\n",
    "    sess.run(init_new_vars_op)\n",
    "else:\n",
    "    print (\"New model initialized\")\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to save our data we need the tf.train.Saver object. When we initialize the Saver without passing arguments to its constructor, by default it will save every variable with the command save that we will later run.\n",
    "Here we show how to save only the variable belonging to the first layer, by passing them in the costructor.\n",
    "The same holds for the initializer object, that can be a global one and working by default on all the variables in the graph, or be called only on some of them.\n",
    "It is a good practice to check the presence of previous checkpoint, before initializing the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9737\n",
      "Model saved in file: check/checkp\n"
     ]
    }
   ],
   "source": [
    "for i in range(EPOCHS):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "    _ = sess.run([train_step], feed_dict={x: batch_xs, y_: batch_ys})     \n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "# Save the variables to disk.\n",
    "save_path = saver.save(sess, \"check/checkp\")\n",
    "print(\"Model saved in file: %s\" % save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are actually saving the state, in the specified path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 2\n",
    "Now let's combine what we have learnt here and in the \"Dataset enrichment\" notebook.\n",
    "Let's create a network and feed it with the data from the CIFAR-10 dataset (by using only 2-3 classes to save training time and see faster results). As a network architecture you can use the same in the \"Dataset enrichment\" notebook, or create one on your own. \n",
    "\n",
    "Our goal in this exercise is to test if there is an effective improvement in using the operations for enriching the dataset. To do so we will:\n",
    "\n",
    "1) Run the network by using the data from the CIFAR-10, without preprocessing, and store the network configuration that performs better on the test set after some epochs of training. To do so, write some code that \n",
    "- evaluate the accuracy on the test set every 100 iterations, and keep the maximum accuracy in memory. \n",
    "- As long as the network improves on the test set, save its current variables. \n",
    "- Stop the training phase when the network is not improving after 400 iterations. \n",
    "\n",
    "This technique is called \"Early stopping\" and is used to prevent overfitting.\n",
    "\n",
    "2) Run the same network, but instead of inizialiting it randomly, use the configuration of the previously trained network. This time, train the network with the enriched dataset and check wether the test prediction has improved.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
