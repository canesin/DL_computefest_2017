{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 2\n",
    "Now let's combine what we have learnt here and in the \"Dataset enrichment\" notebook.\n",
    "Let's create a network (with the features of the Dataset enrichment notebook one) and feed it with the data from the CIFAR-10 dataset (by using only 2-3 classes to save training time and see faster results).\n",
    "\n",
    "Our goal in this exercise is to test if there is an effective improvment in using the operations for enriching the dataset. To do so we will:\n",
    "\n",
    "1) Run the network by using the data from the CIFAR-10, without preprocessing, and store the network configuration that performs better on the test set after some epochs of training. To do so, write some code that \n",
    "- evaluate the accuracy on the test set every 100 iterations, and keep the maximum accuracy in memory. \n",
    "- As long as the network improves on the test set, save its current variables. \n",
    "- Stop the training phase when the network is not improving after 400 iterations. \n",
    "\n",
    "This technique is called \"Early stopping and is used to prevent overfitting\"\n",
    "\n",
    "2) Run the same network, but instead of inizialiting it randomly, use the configuration of the previously trained network. This time, train the network with the enriched dataset and check wether the test prediction has improved.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def conv_weights(filters_size, channels_size, name):\n",
    "    shape = filters_size + channels_size\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1), name=name)\n",
    "\n",
    "def conv(x, W, stride, name):\n",
    "    strides_shape = [1, stride, stride, 1]\n",
    "    return tf.nn.conv2d(x, W, strides_shape, padding='SAME', name=name)\n",
    "\n",
    "def pool(x, size, stride, name):\n",
    "    pool_shape = [1] + size + [1]\n",
    "    strides_shape = [1, stride, stride, 1]\n",
    "    return tf.nn.max_pool(x, pool_shape, strides_shape, padding='SAME', name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import cifar10\n",
    "#Set it true if you want to retrieve data from the checkpoint\n",
    "RESTORE_OPT = True \n",
    "ENRICHED_DATA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Download progress: 100.0%\n",
      "Download finished. Extracting files.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "cifar10.maybe_download_and_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data: data/CIFAR-10/cifar-10-batches-py/batches.meta\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'automobile',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'deer',\n",
       " 'dog',\n",
       " 'frog',\n",
       " 'horse',\n",
       " 'ship',\n",
       " 'truck']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = cifar10.load_class_names()\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cifar10 import  num_channels\n",
    "num_classes=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_1\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_2\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_3\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_4\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_5\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/test_batch\n",
      "Size of:\n",
      "- Training-set:\t\t20000\n",
      "- Test-set:\t\t4000\n",
      "(20000, 32, 32, 3)\n",
      "(20000,)\n",
      "(20000, 10)\n"
     ]
    }
   ],
   "source": [
    "upper_b = (len(class_names)-num_classes)\n",
    "images_train, cls_train, labels_train = cifar10.load_training_data()\n",
    "train_size= len(cls_train)\n",
    "images_train, cls_train, labels_train = images_train[cls_train<num_classes], cls_train[cls_train<num_classes], labels_train[cls_train<num_classes]\n",
    "images_test, cls_test, labels_test = cifar10.load_test_data()\n",
    "test_size= len(cls_test)\n",
    "images_test, cls_test, labels_test = images_test[cls_test<num_classes], cls_test[cls_test<num_classes], labels_test[cls_test<num_classes]\n",
    "print(\"Size of:\")\n",
    "print(\"- Training-set:\\t\\t{}\".format(len(images_train)))\n",
    "print(\"- Test-set:\\t\\t{}\".format(len(images_test)))\n",
    "\n",
    "train_batch_size= 128\n",
    "print (images_train.shape)\n",
    "print (cls_train.shape)\n",
    "print (labels_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_size = 32\n",
    "img_size_cropped = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 4)\n",
      "[[ 0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " ..., \n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giovanni/.local/lib/python3.5/site-packages/ipykernel/__main__.py:1: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  if __name__ == '__main__':\n",
      "/home/giovanni/.local/lib/python3.5/site-packages/ipykernel/__main__.py:3: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "labels_train =  labels_train[0:train_size*num_classes/10,0:-upper_b]\n",
    "\n",
    "labels_test =  labels_test[0:test_size*num_classes/10,0:-upper_b]\n",
    "\n",
    "print (labels_train.shape)\n",
    "print (labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_process_image(image):\n",
    "    image = tf.random_crop(image, size=[img_size_cropped, img_size_cropped, num_channels])\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_hue(image, max_delta=0.5)\n",
    "    image = tf.image.random_contrast(image, lower=0.03, upper=10.0)\n",
    "    image = tf.image.random_brightness(image, max_delta=2)\n",
    "    image = tf.image.random_saturation(image, lower=0.0, upper=20.0)\n",
    "    #Some of these functions may produce an output value beyond the [0,1] range of an image\n",
    "    #To prevent this effect, we limit the range \n",
    "    image = tf.minimum(image, 1.0)\n",
    "    image = tf.maximum(image, 0.0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resize_util(image):\n",
    "    image = tf.image.resize_image_with_crop_or_pad(image,\n",
    "                                                   target_height=img_size_cropped,\n",
    "                                                   target_width=img_size_cropped)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_process(images):\n",
    "    images = tf.map_fn(lambda image: pre_process_image(image), images)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resize(images):\n",
    "    # Use TensorFlow to loop over all the input images and call\n",
    "    # the function above which takes a single image as input.\n",
    "    images = tf.map_fn(lambda image: resize_util(image), images)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_batch(size, training= True):\n",
    "    if training == True:\n",
    "        num_images = len(images_train)\n",
    "    else:\n",
    "        num_images = len(images_test)\n",
    "        \n",
    "    # Create a random index.\n",
    "    idx = np.random.choice(num_images,\n",
    "                           size=size,\n",
    "                           replace=False)\n",
    "    # Use the random index to select random images and labels.\n",
    "    if training == True:\n",
    "        x_batch = images_train[idx, :, :, :]\n",
    "        y_batch = labels_train[idx, :]\n",
    "    else:\n",
    "        x_batch = images_test[idx, :, :, :]\n",
    "        y_batch = labels_test[idx, :]\n",
    "        \n",
    "    return x_batch.astype(np.float32), y_batch.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model initialized\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 0.0005 # BEST: GD Optimizer with lambda=0.5\n",
    "ITERATIONS = 1000\n",
    "\n",
    "input_ =tf.placeholder(tf.float32,[None, 32, 32, 3], name='input_images')\n",
    "if ENRICHED_DATA:\n",
    "    input_images = pre_process(images=input_)\n",
    "else:\n",
    "    input_images = resize(input_)\n",
    "y_ = tf.placeholder(tf.float32, [None, num_classes], name='labels')\n",
    "drop_prob = tf.placeholder(tf.float32, name='drop_prob')\n",
    "\n",
    "W1 = conv_weights([7, 7], [3, 32], 'L1_weights')\n",
    "b1 = tf.Variable(tf.constant(0.1, shape=[32]), name='L1_biases')\n",
    "c1 = conv(input_images, W1, stride=1, name='L1_conv')\n",
    "h1 = tf.nn.relu(tf.nn.bias_add(c1, b1), name='L1_ReLU')\n",
    "p1 = pool(h1, size=[2, 2], stride=2, name='L1_pool')\n",
    " \n",
    "W2 = conv_weights([7,7], [32, 64], 'L2_weights')\n",
    "b2 = tf.Variable(tf.constant(0.1, shape=[64]), name='L2_biases')\n",
    "c2 = conv(p1, W2, stride=1, name='L2_conv')\n",
    "h2 = tf.nn.relu(tf.nn.bias_add(c2, b2), name='L2_ReLU')\n",
    "p2 = pool(h2, size=[2, 2], stride=2, name='L2_pool')\n",
    "\n",
    "\n",
    "W3 = tf.Variable(tf.truncated_normal([7 * 7 * 64, 1024], stddev=1 / math.sqrt(7 * 7 * 64)), name='L3_weights')\n",
    "b3 = tf.Variable(tf.constant(0.1, shape=[1024]), name='L3_biases')\n",
    "p2_flat = tf.reshape(p2, [-1, 7 * 7 * 64])\n",
    "h3 = tf.nn.relu(tf.matmul(p2_flat, W3) + b3)\n",
    "h3_drop = tf.nn.dropout(h3, 1.0 - drop_prob, name='L3_dropout')\n",
    "\n",
    "\n",
    "W4 = tf.Variable(tf.truncated_normal([1024, num_classes], stddev=1 / math.sqrt(1024)), name='L4_weights')\n",
    "b4 = tf.Variable(tf.constant(0.1, shape=[num_classes]), name='L4_biases')\n",
    "y = tf.matmul(h3_drop, W4) + b4\n",
    "\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_), name='cross_entropy')\n",
    "train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(cross_entropy)\n",
    "\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1), name='correct_prediction')\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='train_accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model initialized\n"
     ]
    }
   ],
   "source": [
    "save_dir = 'check/'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "if os.path.exists(\"check/checkpointerfre\") and RESTORE_OPT:\n",
    "    saver.restore(sess, \"check/checkp\")\n",
    "    print (\"Model restored\")\n",
    "else:\n",
    "    print (\"New model initialized\")\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    max_acc = 0\n",
    "    counter = 0\n",
    "    for i in range(ITERATIONS):\n",
    "        batch_xs, batch_ys = random_batch(BATCH_SIZE, training= True)\n",
    "        _ = sess.run([train_step], feed_dict={input_: batch_xs, y_: batch_ys, drop_prob: np.float32(0.5)})\n",
    "  \n",
    "        if i % 50 == 0:\n",
    "            acc = sess.run(accuracy, feed_dict={input_: batch_xs, y_: batch_ys, drop_prob: np.float32(0.0)})\n",
    "            print('%d/%d. training accuracy = %.2f' % (i, ITERATIONS, acc))\n",
    "        if i % 100 == 0:\n",
    "            batch_xs_test, batch_ys_test = random_batch(len(images_test), training= False)\n",
    "            test_acc = sess.run(accuracy, feed_dict={input_: batch_xs_test, y_: batch_ys_test,drop_prob: np.float32(0.0)})\n",
    "            counter = counter + 100\n",
    "            if counter >500:\n",
    "                return \"The test accuracy has stopped increasing\"\n",
    "            else:\n",
    "                if test_acc > max_acc:\n",
    "                    max_acc = test_acc\n",
    "                    counter = 0\n",
    "                    save_path = saver.save(sess, \"check/checkp\")\n",
    "                    print(\"Model saved in file: %s\" % save_path)\n",
    "            print('test accuracy = %.2f' % test_acc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1000. training accuracy = 0.33\n",
      "Model saved in file: check/checkp\n",
      "test accuracy = 0.25\n",
      "50/1000. training accuracy = 0.38\n",
      "100/1000. training accuracy = 0.39\n",
      "Model saved in file: check/checkp\n",
      "test accuracy = 0.44\n",
      "150/1000. training accuracy = 0.51\n",
      "200/1000. training accuracy = 0.48\n",
      "Model saved in file: check/checkp\n",
      "test accuracy = 0.48\n",
      "250/1000. training accuracy = 0.38\n",
      "300/1000. training accuracy = 0.53\n",
      "test accuracy = 0.47\n",
      "350/1000. training accuracy = 0.54\n",
      "400/1000. training accuracy = 0.48\n",
      "Model saved in file: check/checkp\n",
      "test accuracy = 0.52\n",
      "450/1000. training accuracy = 0.53\n",
      "500/1000. training accuracy = 0.59\n",
      "test accuracy = 0.52\n",
      "550/1000. training accuracy = 0.57\n",
      "600/1000. training accuracy = 0.56\n",
      "Model saved in file: check/checkp\n",
      "test accuracy = 0.55\n",
      "650/1000. training accuracy = 0.56\n",
      "700/1000. training accuracy = 0.52\n",
      "Model saved in file: check/checkp\n",
      "test accuracy = 0.56\n",
      "750/1000. training accuracy = 0.58\n",
      "800/1000. training accuracy = 0.57\n",
      "Model saved in file: check/checkp\n",
      "test accuracy = 0.56\n",
      "850/1000. training accuracy = 0.56\n",
      "900/1000. training accuracy = 0.50\n",
      "test accuracy = 0.56\n",
      "950/1000. training accuracy = 0.53\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
